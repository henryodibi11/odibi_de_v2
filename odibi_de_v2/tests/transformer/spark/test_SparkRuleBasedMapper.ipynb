{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c482cb-fcba-4120-893e-f23b5e616a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda3e8eb-0cbf-4b4d-8f53-d5f7c1b6600f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('./odibi_de_v2'))\n",
    "os.environ[\"PYTHONDONTWRITEBYTECODE\"] = \"1\"\n",
    "from odibi_de_v2.transformer import SQLTransformerFromConfig, SparkEventSplitter,SparkRuleBasedMapper\n",
    "from odibi_de_v2.config import ConfigUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0cebad-7e80-4ce3-95fd-210caed02bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Define the schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"OEE_EVENT_TYPE\", StringType(), True),\n",
    "    StructField(\"OEE_UNPLNNED_REASONS\", StringType(), True),\n",
    "    StructField(\"OEE_PLANNED_REASONS\", StringType(), True),\n",
    "    StructField(\"OEE_OFF_REASONS\", StringType(), True),\n",
    "    StructField(\"OEE_EVENT_TARGET\", StringType(), True),\n",
    "    StructField(\"OEE_EVENT_ACTUAL\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Dummy DataFrame\n",
    "data = [\n",
    "    (\"Production\", \"\", \"\", \"\", None, None),\n",
    "    (\"Unplanned Downtime Event\", \"Pump failure\", \"\", \"\", None, None),\n",
    "    (\"Planned Downtime\", \"\", \"Scheduled Maintenance\", \"\", None, None),\n",
    "    (\"Market Driven Down Time\", \"\", \"\", \"Low Demand\", None, None),\n",
    "    (\"Unknown Event\", \"\", \"\", \"\", None, None),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "display(df)\n",
    "\n",
    "# Create the Transformer\n",
    "transformer = SparkRuleBasedMapper(\n",
    "    column_name=\"DETERMINED_REASON\",\n",
    "    rules=[\n",
    "        {\"condition\": \"OEE_EVENT_TYPE == 'Production'\", \"value\": \"Production Event\",\"use_column\": False},\n",
    "        {\"condition\": \"OEE_EVENT_TYPE == 'Unplanned Downtime Event' and OEE_UNPLNNED_REASONS != ''\", \"value\": \"OEE_UNPLNNED_REASONS\",\"use_column\": True},\n",
    "        {\"condition\": \"OEE_EVENT_TYPE == 'Planned Downtime' and OEE_PLANNED_REASONS != ''\", \"value\": \"OEE_PLANNED_REASONS\",\"use_column\": True},\n",
    "        {\"condition\": \"OEE_EVENT_TYPE == 'Market Driven Down Time' and OEE_OFF_REASONS != ''\", \"value\": \"OEE_OFF_REASONS\",\"use_column\": True},\n",
    "    ],\n",
    "    default_value=\"Mismatched Reason\"\n",
    ")\n",
    "\n",
    "# Transform the DataFrame\n",
    "result_df = transformer.transform(df)\n",
    "\n",
    "# Show the results\n",
    "display(result_df.select(\n",
    "    \"OEE_EVENT_TYPE\",\n",
    "    \"OEE_UNPLNNED_REASONS\",\n",
    "    \"OEE_PLANNED_REASONS\",\n",
    "    \"OEE_OFF_REASONS\",\n",
    "    \"DETERMINED_REASON\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b32c2482-928c-4d7c-8c1d-0bd5ccbeba05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Import your upgraded SparkRuleBasedMapper\n",
    "# from spark_rule_based_mapper import SparkRuleBasedMapper\n",
    "\n",
    "# Start Spark\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"TestAllWithDynamic\").getOrCreate()\n",
    "\n",
    "# Sample Data\n",
    "schema = StructType([\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"level\", StringType(), True),\n",
    "])\n",
    "\n",
    "data = [\n",
    "    {\"status\": \"Active\", \"score\": 95, \"name\": \"John Doe\", \"type\": \"Gold\", \"region\": \"North\", \"level\": \"A\"},\n",
    "    {\"status\": \"Inactive\", \"score\": 45, \"name\": \"Jane Smith\", \"type\": \"Silver\", \"region\": \"South\", \"level\": \"B\"},\n",
    "    {\"status\": \"Active\", \"score\": 70, \"name\": \"Johnny Appleseed\", \"type\": \"Bronze\", \"region\": \"West\", \"level\": None},\n",
    "    {\"status\": None, \"score\": 55, \"name\": \"Unknown Name\", \"type\": None, \"region\": \"East\", \"level\": \"C\"},\n",
    "    {\"status\": \"Pending\", \"score\": 30, \"name\": \"Jim Brown\", \"type\": \"Gold\", \"region\": \"South\", \"level\": \"D\"},\n",
    "    {\"status\": \"Active\", \"score\": 85, \"name\": \"Joanna Blue\", \"type\": \"Platinum\", \"region\": \"North\", \"level\": \"A\"},\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Rules\n",
    "rules = [\n",
    "    {\"condition\": \"status == 'Active' AND score > 90\", \"value\": \"Top Performer\", \"use_column\": False},\n",
    "    {\"condition\": \"status != 'Active'\", \"value\": \"Non-Active\", \"use_column\": False},\n",
    "    {\"condition\": \"score >= 60 AND score <= 80\", \"value\": 'lambda df: df[\"score\"] * 1.1', \"use_column\": False},\n",
    "    {\"condition\": \"name like 'John%'\", \"value\": lambda df: F.concat(df[\"name\"], F.lit(\" (Family)\")), \"use_column\": False},\n",
    "    {\"condition\": \"type in ('Gold', 'Silver')\", \"value\": \"Valued Customer\", \"use_column\": False},\n",
    "    {\"condition\": \"region not in ('East', 'West')\", \"value\": \"Preferred Region\", \"use_column\": False},\n",
    "    {\"condition\": \"level is null\", \"value\": \"No Level\", \"use_column\": False},\n",
    "    {\"condition\": \"level is not null\", \"value\": \"Level Exists\", \"use_column\": False},\n",
    "]\n",
    "\n",
    "# Create and apply mapper\n",
    "mapper = SparkRuleBasedMapper(\n",
    "    column_name=\"mapped_category\",\n",
    "    rules=rules,\n",
    "    default_value=\"Other\"\n",
    ")\n",
    "\n",
    "transformed_df = mapper.transform(df)\n",
    "\n",
    "# Show result\n",
    "transformed_df.select(\"status\", \"score\", \"name\", \"type\", \"region\", \"level\", \"mapped_category\").show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5843370131342600,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "test_SparkRuleBasedMapper",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
