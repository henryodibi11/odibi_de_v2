{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c482cb-fcba-4120-893e-f23b5e616a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda3e8eb-0cbf-4b4d-8f53-d5f7c1b6600f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('./odibi_de_v2'))\n",
    "os.environ[\"PYTHONDONTWRITEBYTECODE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75019e75-2151-4ca7-bcd6-c496f49a4fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from odibi_de_v2.transformer import PandasColumnAdder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74ff7715-f03a-43d9-8810-e79a42c64035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Union, Dict, Any, List\n",
    "import pandas as pd\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "\n",
    "from odibi_de_v2.core import Framework, ErrorType\n",
    "from odibi_de_v2.utils import enforce_types, log_call\n",
    "from odibi_de_v2.logger import log_exceptions\n",
    "from odibi_de_v2.transformer import TransformerProvider\n",
    "\n",
    "\n",
    "class TransformerFromConfig:\n",
    "    \"\"\"\n",
    "    Applies one or more transformations to a Pandas or Spark DataFrame using a config-driven approach.\n",
    "\n",
    "    This class delegates to the appropriate transformer via TransformerProvider based on the framework.\n",
    "\n",
    "    Args:\n",
    "        framework (Framework): Enum indicating whether to use Pandas or Spark transformers.\n",
    "\n",
    "    Example Config Format:\n",
    "        [\n",
    "            {\n",
    "                \"transformer\": \"PandasColumnRenamer\",\n",
    "                \"params\": {\"column_map\": {\"old\": \"new\"}}\n",
    "            },\n",
    "            {\n",
    "                \"transformer\": \"PandasColumnDropper\",\n",
    "                \"params\": {\"columns_to_drop\": [\"unnecessary_col\"]}\n",
    "            }\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    @log_call(module=\"TRANSFORMATION\", component=\"TransformerFromConfig\")\n",
    "    @enforce_types(strict=True)\n",
    "    @log_exceptions(\n",
    "        module=\"TRANSFORMATION\",\n",
    "        component=\"TransformerFromConfig\",\n",
    "        error_type=ErrorType.INIT_ERROR,\n",
    "        raise_type=RuntimeError)\n",
    "    def __init__(self, framework: Framework):\n",
    "        self.framework = framework\n",
    "        self.provider = TransformerProvider(framework=framework)\n",
    "\n",
    "    @log_call(module=\"TRANSFORMATION\", component=\"TransformerFromConfig\")\n",
    "    @enforce_types(strict=True)\n",
    "    @log_exceptions(\n",
    "        module=\"TRANSFORMATION\",\n",
    "        component=\"TransformerFromConfig\",\n",
    "        error_type=ErrorType.TRANSFORM_ERROR,\n",
    "        raise_type=RuntimeError)\n",
    "    def transform(\n",
    "        self,\n",
    "        data: Union[pd.DataFrame, SparkDataFrame],\n",
    "        config: list\n",
    "    ) -> Union[pd.DataFrame, SparkDataFrame]:\n",
    "        \"\"\"\n",
    "        Applies one or multiple transformations using config-driven logic.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The input DataFrame.\n",
    "            config (Union[Dict, List[Dict]]): One or more transformer configs.\n",
    "                Each dict must include:\n",
    "                    - \"transformer\": Name of the transformer class\n",
    "                    - \"params\": Dict of keyword arguments for the transformer\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The transformed DataFrame.\n",
    "        \"\"\"\n",
    "        if isinstance(config, dict):\n",
    "            config = [config]\n",
    "\n",
    "        for step in config:\n",
    "            transformer_name = step[\"transformer\"]\n",
    "            transformer_params = step.get(\"params\", {})\n",
    "            data = self.provider.transform(transformer_name, data, **transformer_params)\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a32f5d7-f791-48e5-b3ae-632c5fa89fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from odibi_de_v2.core import Framework\n",
    "# from odibi_de_v2.transformer import TransformerFromConfig\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Setup ----------\n",
    "spark = SparkSession.builder.appName(\"TransformerFromConfigTest\").getOrCreate()\n",
    "spark_df = spark.createDataFrame([(1, \"A\", 3.0), (2, \"B\", 6.0)], [\"col1\", \"col2\", \"col3\"])\n",
    "pandas_df = pd.DataFrame({\"col1\": [1, 2], \"col2\": [\"A\", \"B\"], \"col3\": [3.0, 6.0]})\n",
    "\n",
    "# ---------- Shared Configs ----------\n",
    "column_map = {\"col1\": \"id\", \"col2\": \"name\"}\n",
    "columns_to_drop = [\"col3\"]\n",
    "value_map = {\"name\": {\"A\": \"X\", \"B\": \"Y\"}}\n",
    "column_order = [\"name\", \"id\"]\n",
    "case_style = \"snake_case\"\n",
    "exclude_columns = [\"name\"]\n",
    "\n",
    "# ---------- Pandas Config ----------\n",
    "pandas_configs = [\n",
    "    {\"transformer\": \"PandasColumnRenamer\", \"params\": {\"column_map\": column_map}},\n",
    "    {\"transformer\": \"PandasColumnDropper\", \"params\": {\"columns_to_drop\": columns_to_drop}},\n",
    "    {\"transformer\": \"PandasValueReplacer\", \"params\": {\"value_map\": value_map}},\n",
    "    {\"transformer\": \"PandasColumnReorderer\", \"params\": {\"column_order\": column_order, \"retain_unspecified\": True}},\n",
    "    {\"transformer\": \"PandasColumnAdder\", \"params\": {\"column_name\": \"static_col\", \"value\": \"static\"}},\n",
    "    {\"transformer\": \"PandasColumnNameStandardizer\", \"params\": {\"case_style\": case_style, \"exclude_columns\": exclude_columns}},\n",
    "]\n",
    "\n",
    "# ---------- Spark Config ----------\n",
    "spark_configs = [\n",
    "    {\"transformer\": \"SparkColumnRenamer\", \"params\": {\"column_map\": column_map}},\n",
    "    {\"transformer\": \"SparkColumnDropper\", \"params\": {\"columns_to_drop\": columns_to_drop}},\n",
    "    {\"transformer\": \"SparkValueReplacer\", \"params\": {\"value_map\": value_map}},\n",
    "    {\"transformer\": \"SparkColumnReorderer\", \"params\": {\"column_order\": column_order, \"retain_unspecified\": True}},\n",
    "    {\"transformer\": \"SparkColumnAdder\", \"params\": {\"column_name\": \"static_col\", \"value\": \"static\"}},\n",
    "    {\"transformer\": \"SparkColumnNameStandardizer\", \"params\": {\"case_style\": case_style, \"exclude_columns\": exclude_columns}},\n",
    "]\n",
    "\n",
    "# ---------- Run Pandas ----------\n",
    "print(\"PANDAS RESULTS\")\n",
    "pandas_runner = TransformerFromConfig(framework=Framework.PANDAS)\n",
    "df_pandas_transformed = pandas_runner.transform(pandas_df, pandas_configs)\n",
    "print(df_pandas_transformed)\n",
    "\n",
    "# ---------- Run Spark ----------\n",
    "print(\"SPARK RESULTS\")\n",
    "spark_runner = TransformerFromConfig(framework=Framework.SPARK)\n",
    "df_spark_transformed = spark_runner.transform(spark_df, spark_configs)\n",
    "df_spark_transformed.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "test_transformer_provider_from_config",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
