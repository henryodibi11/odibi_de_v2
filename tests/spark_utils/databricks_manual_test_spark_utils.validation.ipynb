{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7cdc97-a835-48e1-b14b-690cacb38b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('./odibi_de_v2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07c63d9e-c8b9-44a9-95db-8150b6ef9622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, StructType\n",
    "from odibi_de_v2.spark_utils.validation import (\n",
    "   is_spark_dataframe,\n",
    "   has_columns,\n",
    "   is_empty,\n",
    "   has_nulls_in_columns,\n",
    "   has_duplicate_columns,\n",
    "   is_flat_dataframe\n",
    ")\n",
    "\n",
    "# Use the shared SparkSession in Databricks\n",
    "spark = spark if \"spark\" in globals() else SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define common schema\n",
    "schema = StructType([\n",
    "   StructField(\"id\", IntegerType(), True),\n",
    "   StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Test DataFrames\n",
    "df_valid = spark.createDataFrame([(1, \"A\")], schema)\n",
    "df_invalid = \"not_a_dataframe\"\n",
    "\n",
    "df_empty = spark.createDataFrame([], schema)\n",
    "df_nulls = spark.createDataFrame([(1, None), (2, \"B\")], schema)\n",
    "\n",
    "df_duplicates = spark.createDataFrame([(1, 2)], [\"a\", \"a\"])\n",
    "df_clean = spark.createDataFrame([(1, 2)], [\"x\", \"y\"])\n",
    "\n",
    "df_nested = spark.read.json(spark.sparkContext.parallelize([\n",
    "   '{\"id\": 1, \"info\": {\"score\": 10}}'\n",
    "]))\n",
    "\n",
    "# Run Tests\n",
    "print(\"is_spark_dataframe (valid):\", is_spark_dataframe(df_valid))       # True\n",
    "print(\"is_spark_dataframe (invalid):\", is_spark_dataframe(df_invalid))   # False\n",
    "\n",
    "print(\"has_columns ['id']:\", has_columns(df_valid, [\"id\"]))              # True\n",
    "print(\"has_columns ['missing']:\", has_columns(df_valid, [\"missing\"]))    # False\n",
    "\n",
    "print(\"is_empty (True):\", is_empty(df_empty))                            # True\n",
    "print(\"is_empty (False):\", is_empty(df_valid))                           # False\n",
    "\n",
    "print(\"has_nulls_in_columns ['name']:\", has_nulls_in_columns(df_nulls, [\"name\"]))  # True\n",
    "print(\"has_nulls_in_columns ['id']:\", has_nulls_in_columns(df_nulls, [\"id\"]))      # False\n",
    "\n",
    "print(\"has_duplicate_columns (True):\", has_duplicate_columns(df_duplicates))       # True\n",
    "print(\"has_duplicate_columns (False):\", has_duplicate_columns(df_clean))           # False\n",
    "\n",
    "print(\"is_flat_dataframe (True):\", is_flat_dataframe(df_valid))          # True\n",
    "print(\"is_flat_dataframe (False):\", is_flat_dataframe(df_nested))        # False"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_manual_test_spark_utils.validation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
