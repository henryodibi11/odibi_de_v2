{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c482cb-fcba-4120-893e-f23b5e616a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda3e8eb-0cbf-4b4d-8f53-d5f7c1b6600f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('./odibi_de_v2'))\n",
    "os.environ[\"PYTHONDONTWRITEBYTECODE\"] = \"1\"\n",
    "from odibi_de_v2.connector import AzureBlobConnection, LocalConnection\n",
    "from odibi_de_v2.ingestion import SparkStreamingDataReader\n",
    "from odibi_de_v2.storage import SparkStreamingDataSaver\n",
    "from odibi_de_v2.core import DataType\n",
    "from odibi_de_v2.core import Framework\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "265ec9f8-8c64-489d-a396-d70904881a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, DataFrame as SparkDataFrame\n",
    "\n",
    "from odibi_de_v2.core.enums import DataType, Framework\n",
    "from odibi_de_v2.core import BaseConnection\n",
    "from odibi_de_v2.connector import LocalConnection\n",
    "from odibi_de_v2.ingestion.spark.spark_data_reader import SparkDataReader\n",
    "from odibi_de_v2.ingestion.spark.spark_streaming_data_reader import SparkStreamingDataReader\n",
    "from odibi_de_v2.ingestion.pandas.pandas_data_reader import PandasDataReader\n",
    "\n",
    "\n",
    "class ReaderProvider:\n",
    "    \"\"\"\n",
    "    Unified provider for reading data using either Pandas, Spark, or Spark Streaming.\n",
    "\n",
    "    Uses the framework and optional is_stream flag to determine behavior.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        connector: Optional[BaseConnection] = None,\n",
    "        local_engine: Framework = Framework.PANDAS):\n",
    "        self.connector = connector or LocalConnection()\n",
    "        self.local_engine = local_engine\n",
    "\n",
    "    def read(\n",
    "        self,\n",
    "        data_type: DataType,\n",
    "        container: str,\n",
    "        path_prefix: str,\n",
    "        object_name: str,\n",
    "        spark: Optional[SparkSession] = None,\n",
    "        is_stream: Optional[bool] = False,\n",
    "        **kwargs\n",
    "    ) -> Union[pd.DataFrame, SparkDataFrame]:\n",
    "        \"\"\"\n",
    "        Reads data from local or cloud sources using the appropriate engine.\n",
    "\n",
    "        Args:\n",
    "            data_type (DataType): File format (CSV, JSON, etc.).\n",
    "            container (str): Top-level bucket/container.\n",
    "            path_prefix (str): Folder inside the container.\n",
    "            object_name (str): File name.\n",
    "            spark (SparkSession, optional): Required for Spark-based reads.\n",
    "            is_stream (bool, optional): For Spark only; triggers streaming.\n",
    "            **kwargs: Reader-specific options.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame or Spark DataFrame\n",
    "        \"\"\"\n",
    "        if not self.connector:\n",
    "            raise ValueError(\"Connector is required to resolve paths.\")\n",
    "\n",
    "        file_path = self.connector.get_file_path(\n",
    "            container=container,\n",
    "            path_prefix=path_prefix,\n",
    "            object_name=object_name\n",
    "        )\n",
    "\n",
    "        options = self.connector.get_storage_options()\n",
    "\n",
    "        framework = self.connector.framework\n",
    "        if framework == Framework.LOCAL:\n",
    "            framework = self.local_engine\n",
    "\n",
    "        match framework:\n",
    "            case Framework.PANDAS:\n",
    "                reader = PandasDataReader()\n",
    "                return reader.read_data(\n",
    "                    data_type=data_type,\n",
    "                    file_path=file_path,\n",
    "                    storage_options=options,\n",
    "                    **kwargs\n",
    "                )\n",
    "\n",
    "            case Framework.SPARK:\n",
    "                if spark is None:\n",
    "                    raise ValueError(\"Spark session is required for Spark reads.\")\n",
    "\n",
    "                if options:\n",
    "                    for key, value in options.items():\n",
    "                        spark.conf.set(key, value)\n",
    "\n",
    "                reader = SparkStreamingDataReader() if is_stream else SparkDataReader()\n",
    "                return reader.read_data(\n",
    "                    data_type=data_type,\n",
    "                    file_path=file_path,\n",
    "                    spark=spark,\n",
    "                    **kwargs\n",
    "                )\n",
    "\n",
    "            case _:\n",
    "                raise NotImplementedError(f\"Framework {self.connector.framework} is not supported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "469e1601-67ea-4af8-9232-fab04ee6fcc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "azure_spark_connector = AzureBlobConnection(\n",
    "    dbutils.secrets.get(\"scope\", \"account_name\"),\n",
    "    dbutils.secrets.get(\"scope\", \"account_key\"),\n",
    "    Framework.PANDAS)\n",
    "\n",
    "provider = ReaderProvider(local_engine=Framework.SPARK)\n",
    "\n",
    "df = provider.read(\n",
    "    spark=spark,\n",
    "    data_type=DataType.JSON,\n",
    "    container=\"\",\n",
    "    path_prefix=\"dbfs:/FileStore\",\n",
    "    object_name=\"Calendar.json\",\n",
    "    is_stream=False\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "test_reader_provider",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
