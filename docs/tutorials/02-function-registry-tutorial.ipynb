{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Function Registry and Decorators\n",
    "\n",
    "Learn how to register reusable functions for both Spark and Pandas engines using the ODIBI function framework.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Use `@odibi_function` decorator for function registration\n",
    "- Create engine-specific variants (Spark vs Pandas)\n",
    "- Resolve functions from the global registry\n",
    "- Understand the function resolution order\n",
    "- Use shorthand decorators (`@spark_function`, `@pandas_function`, `@universal_function`)\n",
    "\n",
    "**Prerequisites:**\n",
    "- odibi_de_v2 installed\n",
    "- pandas installed (Spark optional for full examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Function Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi_de_v2.odibi_functions import odibi_function, REGISTRY\n",
    "import pandas as pd\n",
    "\n",
    "# Register a universal function (works with any engine)\n",
    "@odibi_function(\n",
    "    engine=\"any\",\n",
    "    module=\"cleaning\",\n",
    "    description=\"Remove duplicate rows from DataFrame\",\n",
    "    version=\"1.0\"\n",
    ")\n",
    "def remove_duplicates(df):\n",
    "    \"\"\"Remove duplicate rows - works for both Spark and Pandas.\"\"\"\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "print(\"✓ Registered 'remove_duplicates' function\")\n",
    "\n",
    "# Verify registration\n",
    "all_functions = REGISTRY.get_all()\n",
    "print(f\"\\nRegistered functions: {all_functions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve and use the function\n",
    "fn = REGISTRY.resolve(\n",
    "    module=\"cleaning\",\n",
    "    func=\"remove_duplicates\",\n",
    "    engine=\"pandas\"\n",
    ")\n",
    "\n",
    "# Test with sample data\n",
    "test_df = pd.DataFrame({\n",
    "    'id': [1, 2, 2, 3, 4, 4],\n",
    "    'value': ['a', 'b', 'b', 'c', 'd', 'd']\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(test_df)\n",
    "\n",
    "result = fn(test_df)\n",
    "print(\"\\nAfter remove_duplicates:\")\n",
    "print(result)\n",
    "print(f\"\\nRows: {len(test_df)} → {len(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Engine-Specific Implementations\n",
    "\n",
    "Create different implementations for Spark and Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas-specific implementation\n",
    "@odibi_function(\n",
    "    engine=\"pandas\",\n",
    "    module=\"aggregation\",\n",
    "    description=\"Calculate daily statistics using Pandas\"\n",
    ")\n",
    "def calculate_daily_stats_pandas(df):\n",
    "    \"\"\"Aggregate data by date using pandas groupby.\"\"\"\n",
    "    return df.groupby('date').agg({\n",
    "        'value': ['count', 'sum', 'mean'],\n",
    "        'id': 'nunique'\n",
    "    }).reset_index()\n",
    "\n",
    "# Spark-specific implementation (same logical operation)\n",
    "@odibi_function(\n",
    "    engine=\"spark\",\n",
    "    module=\"aggregation\",\n",
    "    description=\"Calculate daily statistics using Spark SQL\"\n",
    ")\n",
    "def calculate_daily_stats_spark(df):\n",
    "    \"\"\"Aggregate data by date using Spark SQL functions.\"\"\"\n",
    "    from pyspark.sql import functions as F\n",
    "    \n",
    "    return df.groupBy('date').agg(\n",
    "        F.count('value').alias('value_count'),\n",
    "        F.sum('value').alias('value_sum'),\n",
    "        F.mean('value').alias('value_mean'),\n",
    "        F.countDistinct('id').alias('id_nunique')\n",
    "    )\n",
    "\n",
    "print(\"✓ Registered engine-specific implementations\")\n",
    "print(\"  - calculate_daily_stats_pandas (engine='pandas')\")\n",
    "print(\"  - calculate_daily_stats_spark (engine='spark')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Function Resolution\n",
    "\n",
    "**Resolution Order:**\n",
    "1. Exact match: `(function_name, engine)`\n",
    "2. Universal fallback: `(function_name, \"any\")`\n",
    "3. None if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test resolution for Pandas engine\n",
    "pandas_fn = REGISTRY.resolve(\n",
    "    module=\"aggregation\",\n",
    "    func=\"calculate_daily_stats_pandas\",\n",
    "    engine=\"pandas\"\n",
    ")\n",
    "\n",
    "print(f\"Resolved for pandas: {pandas_fn.__name__}\")\n",
    "\n",
    "# Test with sample data\n",
    "sample_df = pd.DataFrame({\n",
    "    'date': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02', '2024-01-03'],\n",
    "    'id': [1, 2, 1, 3, 2],\n",
    "    'value': [10, 20, 15, 25, 30]\n",
    "})\n",
    "\n",
    "print(\"\\nSample Data:\")\n",
    "print(sample_df)\n",
    "\n",
    "stats = pandas_fn(sample_df)\n",
    "print(\"\\nDaily Statistics:\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate fallback resolution\n",
    "@odibi_function(\n",
    "    engine=\"any\",\n",
    "    module=\"validation\",\n",
    "    description=\"Count null values\"\n",
    ")\n",
    "def count_nulls(df):\n",
    "    \"\"\"Universal null counter - works with both engines.\"\"\"\n",
    "    return df.isnull().sum()\n",
    "\n",
    "# Resolve for pandas (no pandas-specific version, falls back to 'any')\n",
    "resolved_fn = REGISTRY.resolve(\n",
    "    module=\"validation\",\n",
    "    func=\"count_nulls\",\n",
    "    engine=\"pandas\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Resolved 'count_nulls' for pandas engine: {resolved_fn.__name__}\")\n",
    "\n",
    "# Test\n",
    "test_df_with_nulls = pd.DataFrame({\n",
    "    'a': [1, 2, None, 4],\n",
    "    'b': [None, 'x', 'y', None],\n",
    "    'c': [1.0, 2.0, 3.0, 4.0]\n",
    "})\n",
    "\n",
    "null_counts = resolved_fn(test_df_with_nulls)\n",
    "print(\"\\nNull Counts:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Shorthand Decorators\n",
    "\n",
    "Use convenient aliases for common patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi_de_v2.odibi_functions import (\n",
    "    spark_function,\n",
    "    pandas_function,\n",
    "    universal_function\n",
    ")\n",
    "\n",
    "# Equivalent to @odibi_function(engine=\"pandas\", ...)\n",
    "@pandas_function(\n",
    "    module=\"enrichment\",\n",
    "    description=\"Add revenue column\",\n",
    "    author=\"data_team\"\n",
    ")\n",
    "def add_revenue_column(df):\n",
    "    \"\"\"Calculate revenue from quantity and price.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['revenue'] = df['quantity'] * df['price']\n",
    "    return df\n",
    "\n",
    "# Equivalent to @odibi_function(engine=\"any\", ...)\n",
    "@universal_function(\n",
    "    module=\"enrichment\",\n",
    "    description=\"Add timestamp column\"\n",
    ")\n",
    "def add_timestamp(df):\n",
    "    \"\"\"Add current timestamp to DataFrame.\"\"\"\n",
    "    from datetime import datetime\n",
    "    df = df.copy()\n",
    "    df['processed_at'] = datetime.now()\n",
    "    return df\n",
    "\n",
    "print(\"✓ Registered functions using shorthand decorators:\")\n",
    "print(\"  - add_revenue_column (@pandas_function)\")\n",
    "print(\"  - add_timestamp (@universal_function)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enrichment functions\n",
    "sales_df = pd.DataFrame({\n",
    "    'product': ['Widget A', 'Widget B', 'Widget C'],\n",
    "    'quantity': [10, 5, 8],\n",
    "    'price': [25.50, 40.00, 15.75]\n",
    "})\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(sales_df)\n",
    "\n",
    "# Apply functions directly (they're still normal Python functions)\n",
    "enriched = add_revenue_column(sales_df)\n",
    "enriched = add_timestamp(enriched)\n",
    "\n",
    "print(\"\\nEnriched Data:\")\n",
    "print(enriched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Advanced - Function Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@odibi_function(\n",
    "    engine=\"pandas\",\n",
    "    module=\"quality\",\n",
    "    description=\"Data quality checker with configurable thresholds\",\n",
    "    author=\"quality_team\",\n",
    "    version=\"2.0\",\n",
    "    tags=[\"quality\", \"validation\", \"monitoring\"],\n",
    "    category=\"data_quality\"\n",
    ")\n",
    "def check_data_quality(df, null_threshold=0.1, dup_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality check.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        null_threshold: Max allowed null percentage (default 10%)\n",
    "        dup_threshold: Max allowed duplicate percentage (default 5%)\n",
    "    \n",
    "    Returns:\n",
    "        Quality report dictionary\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    null_pct = (df.isnull().sum().sum() / (total_rows * len(df.columns)))\n",
    "    dup_count = df.duplicated().sum()\n",
    "    dup_pct = dup_count / total_rows if total_rows > 0 else 0\n",
    "    \n",
    "    report = {\n",
    "        'total_rows': total_rows,\n",
    "        'total_columns': len(df.columns),\n",
    "        'null_percentage': null_pct,\n",
    "        'duplicate_count': dup_count,\n",
    "        'duplicate_percentage': dup_pct,\n",
    "        'null_check_passed': null_pct <= null_threshold,\n",
    "        'dup_check_passed': dup_pct <= dup_threshold,\n",
    "        'overall_passed': (null_pct <= null_threshold) and (dup_pct <= dup_threshold)\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Retrieve metadata about the function\n",
    "metadata = REGISTRY.get_metadata(\"check_data_quality\", \"pandas\")\n",
    "print(\"Function Metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test quality checker\n",
    "test_data = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Charlie', None, 'Eve'],\n",
    "    'score': [95, 87, None, None, 92, 88]\n",
    "})\n",
    "\n",
    "quality_report = check_data_quality(test_data)\n",
    "\n",
    "print(\"\\nData Quality Report:\")\n",
    "print(f\"Total Rows: {quality_report['total_rows']}\")\n",
    "print(f\"Null %: {quality_report['null_percentage']:.2%}\")\n",
    "print(f\"Duplicate %: {quality_report['duplicate_percentage']:.2%}\")\n",
    "print(f\"\\n✓ Overall Quality: {'PASSED' if quality_report['overall_passed'] else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Listing and Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all registered functions\n",
    "all_functions = REGISTRY.get_all()\n",
    "\n",
    "print(\"All Registered Functions:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for func_name, engine in sorted(all_functions):\n",
    "    metadata = REGISTRY.get_metadata(func_name, engine)\n",
    "    module = metadata.get('module', 'N/A')\n",
    "    desc = metadata.get('description', 'No description')[:50]\n",
    "    print(f\"\\n{func_name} [engine={engine}]\")\n",
    "    print(f\"  Module: {module}\")\n",
    "    print(f\"  Description: {desc}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by module\n",
    "def get_functions_by_module(module_name):\n",
    "    \"\"\"Get all functions in a specific module.\"\"\"\n",
    "    results = []\n",
    "    for func_name, engine in REGISTRY.get_all():\n",
    "        metadata = REGISTRY.get_metadata(func_name, engine)\n",
    "        if metadata.get('module') == module_name:\n",
    "            results.append((func_name, engine, metadata))\n",
    "    return results\n",
    "\n",
    "# Find all cleaning functions\n",
    "cleaning_functions = get_functions_by_module('cleaning')\n",
    "\n",
    "print(f\"\\nFunctions in 'cleaning' module:\")\n",
    "for func_name, engine, metadata in cleaning_functions:\n",
    "    print(f\"  - {func_name} [{engine}]: {metadata.get('description', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What You Learned:**\n",
    "\n",
    "1. ✓ **@odibi_function Decorator**: Register functions with `engine`, `module`, and metadata\n",
    "2. ✓ **Engine-Specific Variants**: Create Spark and Pandas implementations of same logic\n",
    "3. ✓ **Function Resolution**: Use `REGISTRY.resolve(module, func, engine)` with fallback\n",
    "4. ✓ **Shorthand Decorators**: `@spark_function`, `@pandas_function`, `@universal_function`\n",
    "5. ✓ **Metadata**: Attach author, version, tags, and custom fields\n",
    "6. ✓ **Discovery**: List and filter registered functions programmatically\n",
    "\n",
    "**Key Patterns:**\n",
    "\n",
    "```python\n",
    "# Register function\n",
    "@odibi_function(engine=\"pandas\", module=\"mymodule\")\n",
    "def my_function(df):\n",
    "    return df\n",
    "\n",
    "# Resolve and use\n",
    "fn = REGISTRY.resolve(\"mymodule\", \"my_function\", \"pandas\")\n",
    "result = fn(data)\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- Use `engine=\"any\"` for universal functions that work with both Spark and Pandas\n",
    "- Organize functions by `module` for better discoverability\n",
    "- Add descriptive metadata for documentation and searchability\n",
    "- Functions remain normal Python callables (backward-compatible)\n",
    "\n",
    "**Next Steps:**\n",
    "- Tutorial 3: Hooks and Observability\n",
    "- Tutorial 4: Complete Project Template"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
