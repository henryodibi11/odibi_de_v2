{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Complete Project Setup Template\n",
    "\n",
    "A comprehensive guide to setting up a new data engineering project using odibi_de_v2 with both Spark and Pandas engines.\n",
    "\n",
    "**What You'll Build:**\n",
    "- Complete project structure\n",
    "- Configuration management\n",
    "- Reusable transformation functions\n",
    "- Medallion architecture (Bronze → Silver → Gold)\n",
    "- Observability and monitoring\n",
    "- Both Spark and Pandas implementations\n",
    "\n",
    "**Prerequisites:**\n",
    "- odibi_de_v2 installed\n",
    "- Completion of Tutorials 1-3 recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Customer Analytics Pipeline\n",
    "\n",
    "**Business Goal:** Analyze customer purchase behavior to identify high-value customers and trends.\n",
    "\n",
    "**Data Flow:**\n",
    "- **Bronze**: Raw transaction and customer data\n",
    "- **Silver**: Cleaned, deduplicated, and enriched data\n",
    "- **Gold**: Aggregated customer metrics and KPIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project Structure Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Create project structure\n",
    "project_root = Path(\"./customer_analytics_project\")\n",
    "\n",
    "directories = [\n",
    "    \"config\",\n",
    "    \"data/bronze\",\n",
    "    \"data/silver\",\n",
    "    \"data/gold\",\n",
    "    \"functions/bronze\",\n",
    "    \"functions/silver\",\n",
    "    \"functions/gold\",\n",
    "    \"notebooks\",\n",
    "    \"tests\",\n",
    "    \"logs\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    (project_root / directory).mkdir(parents=True, exist_ok=True)\n",
    "    # Create __init__.py for Python packages\n",
    "    if \"functions\" in directory:\n",
    "        (project_root / directory / \"__init__.py\").touch()\n",
    "\n",
    "print(\"✓ Project structure created:\")\n",
    "print(f\"\\n{project_root}/\")\n",
    "for dir_path in sorted(directories):\n",
    "    print(f\"├── {dir_path}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate synthetic transaction data\n",
    "np.random.seed(42)\n",
    "\n",
    "n_transactions = 200\n",
    "start_date = datetime(2024, 1, 1)\n",
    "\n",
    "transactions = pd.DataFrame({\n",
    "    'transaction_id': range(1, n_transactions + 1),\n",
    "    'customer_id': np.random.randint(1001, 1051, n_transactions),\n",
    "    'product_id': np.random.choice(['P001', 'P002', 'P003', 'P004', 'P005'], n_transactions),\n",
    "    'quantity': np.random.randint(1, 10, n_transactions),\n",
    "    'price': np.random.choice([19.99, 29.99, 49.99, 99.99, 149.99], n_transactions),\n",
    "    'transaction_date': [start_date + timedelta(days=np.random.randint(0, 90)) for _ in range(n_transactions)],\n",
    "    'status': np.random.choice(['completed', 'completed', 'completed', 'cancelled'], n_transactions)\n",
    "})\n",
    "\n",
    "# Add some duplicates (data quality issue)\n",
    "transactions = pd.concat([transactions, transactions.sample(5)], ignore_index=True)\n",
    "\n",
    "# Add some nulls (data quality issue)\n",
    "transactions.loc[transactions.sample(3).index, 'product_id'] = None\n",
    "\n",
    "transactions.to_csv(project_root / \"data/bronze/transactions_raw.csv\", index=False)\n",
    "\n",
    "print(f\"✓ Created transactions data: {len(transactions)} rows\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(transactions.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate customer master data\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': range(1001, 1051),\n",
    "    'customer_name': [f'Customer {i}' for i in range(1001, 1051)],\n",
    "    'segment': np.random.choice(['Premium', 'Standard', 'Basic'], 50),\n",
    "    'join_date': [start_date - timedelta(days=np.random.randint(365, 1095)) for _ in range(50)]\n",
    "})\n",
    "\n",
    "customers.to_csv(project_root / \"data/bronze/customers_raw.csv\", index=False)\n",
    "\n",
    "print(f\"✓ Created customer data: {len(customers)} rows\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(customers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./customer_analytics_project/functions/silver/__init__.py\n",
    "from .transactions import clean_transactions\n",
    "from .customers import clean_customers\n",
    "\n",
    "__all__ = ['clean_transactions', 'clean_customers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./customer_analytics_project/functions/silver/transactions.py\n",
    "\"\"\"\n",
    "Silver layer transformations for transactions.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "from odibi_de_v2.odibi_functions import pandas_function\n",
    "\n",
    "\n",
    "@pandas_function(\n",
    "    module=\"silver.transactions\",\n",
    "    description=\"Clean and enrich transaction data\",\n",
    "    version=\"1.0\"\n",
    ")\n",
    "def clean_transactions(df: pd.DataFrame, constants: Dict[str, Any] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean transaction data:\n",
    "    - Remove duplicates\n",
    "    - Filter out cancelled transactions\n",
    "    - Remove records with null product_id\n",
    "    - Calculate revenue\n",
    "    - Add derived fields\n",
    "    \n",
    "    Args:\n",
    "        df: Raw transaction DataFrame\n",
    "        constants: Optional configuration parameters\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    # Remove duplicates\n",
    "    df_clean = df.drop_duplicates(subset=['transaction_id'])\n",
    "    \n",
    "    # Filter completed transactions only\n",
    "    df_clean = df_clean[df_clean['status'] == 'completed']\n",
    "    \n",
    "    # Remove nulls\n",
    "    df_clean = df_clean.dropna(subset=['product_id'])\n",
    "    \n",
    "    # Calculate revenue\n",
    "    df_clean['revenue'] = df_clean['quantity'] * df_clean['price']\n",
    "    \n",
    "    # Convert date\n",
    "    df_clean['transaction_date'] = pd.to_datetime(df_clean['transaction_date'])\n",
    "    \n",
    "    # Add derived fields\n",
    "    df_clean['year'] = df_clean['transaction_date'].dt.year\n",
    "    df_clean['month'] = df_clean['transaction_date'].dt.month\n",
    "    df_clean['quarter'] = df_clean['transaction_date'].dt.quarter\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Spark variant (if needed)\n",
    "try:\n",
    "    from pyspark.sql import DataFrame\n",
    "    from pyspark.sql import functions as F\n",
    "    from odibi_de_v2.odibi_functions import spark_function\n",
    "    \n",
    "    @spark_function(\n",
    "        module=\"silver.transactions\",\n",
    "        description=\"Clean and enrich transaction data (Spark)\",\n",
    "        version=\"1.0\"\n",
    "    )\n",
    "    def clean_transactions_spark(df: DataFrame, constants: Dict[str, Any] = None) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Spark version of clean_transactions.\n",
    "        \"\"\"\n",
    "        # Remove duplicates\n",
    "        df_clean = df.dropDuplicates(['transaction_id'])\n",
    "        \n",
    "        # Filter completed transactions\n",
    "        df_clean = df_clean.filter(F.col('status') == 'completed')\n",
    "        \n",
    "        # Remove nulls\n",
    "        df_clean = df_clean.filter(F.col('product_id').isNotNull())\n",
    "        \n",
    "        # Calculate revenue\n",
    "        df_clean = df_clean.withColumn('revenue', F.col('quantity') * F.col('price'))\n",
    "        \n",
    "        # Convert date\n",
    "        df_clean = df_clean.withColumn('transaction_date', F.to_date('transaction_date'))\n",
    "        \n",
    "        # Add derived fields\n",
    "        df_clean = df_clean.withColumn('year', F.year('transaction_date'))\n",
    "        df_clean = df_clean.withColumn('month', F.month('transaction_date'))\n",
    "        df_clean = df_clean.withColumn('quarter', F.quarter('transaction_date'))\n",
    "        \n",
    "        return df_clean\n",
    "        \n",
    "except ImportError:\n",
    "    pass  # Spark not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./customer_analytics_project/functions/silver/customers.py\n",
    "\"\"\"\n",
    "Silver layer transformations for customers.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "from odibi_de_v2.odibi_functions import pandas_function\n",
    "\n",
    "\n",
    "@pandas_function(\n",
    "    module=\"silver.customers\",\n",
    "    description=\"Clean and enrich customer data\",\n",
    "    version=\"1.0\"\n",
    ")\n",
    "def clean_customers(df: pd.DataFrame, constants: Dict[str, Any] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean customer data:\n",
    "    - Standardize segment names\n",
    "    - Calculate customer tenure\n",
    "    - Add customer age segments\n",
    "    \n",
    "    Args:\n",
    "        df: Raw customer DataFrame\n",
    "        constants: Optional configuration parameters\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert date\n",
    "    df_clean['join_date'] = pd.to_datetime(df_clean['join_date'])\n",
    "    \n",
    "    # Calculate tenure in days\n",
    "    current_date = pd.Timestamp.now()\n",
    "    df_clean['tenure_days'] = (current_date - df_clean['join_date']).dt.days\n",
    "    \n",
    "    # Tenure segments\n",
    "    df_clean['tenure_segment'] = pd.cut(\n",
    "        df_clean['tenure_days'],\n",
    "        bins=[0, 365, 730, float('inf')],\n",
    "        labels=['New', 'Established', 'Loyal']\n",
    "    )\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./customer_analytics_project/functions/gold/__init__.py\n",
    "from .customer_metrics import calculate_customer_metrics\n",
    "\n",
    "__all__ = ['calculate_customer_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./customer_analytics_project/functions/gold/customer_metrics.py\n",
    "\"\"\"\n",
    "Gold layer aggregations for customer analytics.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "from odibi_de_v2.odibi_functions import pandas_function\n",
    "\n",
    "\n",
    "@pandas_function(\n",
    "    module=\"gold.customer_metrics\",\n",
    "    description=\"Calculate customer-level KPIs\",\n",
    "    version=\"1.0\"\n",
    ")\n",
    "def calculate_customer_metrics(df: pd.DataFrame, constants: Dict[str, Any] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate customer-level metrics from transaction data:\n",
    "    - Total revenue per customer\n",
    "    - Transaction count\n",
    "    - Average order value\n",
    "    - First and last purchase dates\n",
    "    - Recency, Frequency, Monetary (RFM) scores\n",
    "    \n",
    "    Args:\n",
    "        df: Silver layer transaction DataFrame\n",
    "        constants: Optional configuration parameters\n",
    "    \n",
    "    Returns:\n",
    "        Customer metrics DataFrame\n",
    "    \"\"\"\n",
    "    # Aggregate by customer\n",
    "    customer_metrics = df.groupby('customer_id').agg({\n",
    "        'transaction_id': 'count',\n",
    "        'revenue': ['sum', 'mean', 'max'],\n",
    "        'quantity': 'sum',\n",
    "        'transaction_date': ['min', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    customer_metrics.columns = [\n",
    "        'customer_id',\n",
    "        'total_transactions',\n",
    "        'total_revenue',\n",
    "        'avg_order_value',\n",
    "        'max_order_value',\n",
    "        'total_quantity',\n",
    "        'first_purchase_date',\n",
    "        'last_purchase_date'\n",
    "    ]\n",
    "    \n",
    "    # Calculate recency (days since last purchase)\n",
    "    current_date = pd.Timestamp.now()\n",
    "    customer_metrics['recency_days'] = (\n",
    "        current_date - customer_metrics['last_purchase_date']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Calculate customer lifetime (days between first and last purchase)\n",
    "    customer_metrics['customer_lifetime_days'] = (\n",
    "        customer_metrics['last_purchase_date'] - customer_metrics['first_purchase_date']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Customer value segment (based on total revenue)\n",
    "    customer_metrics['value_segment'] = pd.qcut(\n",
    "        customer_metrics['total_revenue'],\n",
    "        q=3,\n",
    "        labels=['Low', 'Medium', 'High']\n",
    "    )\n",
    "    \n",
    "    return customer_metrics.sort_values('total_revenue', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from odibi_de_v2.config import TransformationConfig\n",
    "\n",
    "# Silver Layer Configurations\n",
    "silver_transactions_config = TransformationConfig(\n",
    "    project=\"customer_analytics\",\n",
    "    plant=\"all\",\n",
    "    asset=\"transactions\",\n",
    "    module=\"functions.silver.transactions\",\n",
    "    function=\"clean_transactions\",\n",
    "    input_table=str(project_root / \"data/bronze/transactions_raw.csv\"),\n",
    "    target_table=str(project_root / \"data/silver/transactions_clean.csv\"),\n",
    "    enabled=True,\n",
    "    env=\"dev\",\n",
    "    layer=\"silver\"\n",
    ")\n",
    "\n",
    "silver_customers_config = TransformationConfig(\n",
    "    project=\"customer_analytics\",\n",
    "    plant=\"all\",\n",
    "    asset=\"customers\",\n",
    "    module=\"functions.silver.customers\",\n",
    "    function=\"clean_customers\",\n",
    "    input_table=str(project_root / \"data/bronze/customers_raw.csv\"),\n",
    "    target_table=str(project_root / \"data/silver/customers_clean.csv\"),\n",
    "    enabled=True,\n",
    "    env=\"dev\",\n",
    "    layer=\"silver\"\n",
    ")\n",
    "\n",
    "# Gold Layer Configuration\n",
    "gold_metrics_config = TransformationConfig(\n",
    "    project=\"customer_analytics\",\n",
    "    plant=\"all\",\n",
    "    asset=\"customer_metrics\",\n",
    "    module=\"functions.gold.customer_metrics\",\n",
    "    function=\"calculate_customer_metrics\",\n",
    "    input_table=str(project_root / \"data/silver/transactions_clean.csv\"),\n",
    "    target_table=str(project_root / \"data/gold/customer_metrics.csv\"),\n",
    "    enabled=True,\n",
    "    env=\"dev\",\n",
    "    layer=\"gold\"\n",
    ")\n",
    "\n",
    "print(\"✓ Created transformation configurations\")\n",
    "print(\"\\nPipeline stages:\")\n",
    "print(\"  1. Silver - Clean Transactions\")\n",
    "print(\"  2. Silver - Clean Customers\")\n",
    "print(\"  3. Gold - Customer Metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Setup Observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi_de_v2.hooks import HookManager\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Create hooks manager\n",
    "project_hooks = HookManager()\n",
    "\n",
    "# Audit log file\n",
    "audit_log_path = project_root / \"logs/pipeline_audit.log\"\n",
    "\n",
    "def audit_logger(payload):\n",
    "    \"\"\"Log all pipeline events to audit file.\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    event = payload.get('event', 'unknown')\n",
    "    layer = payload.get('layer', 'N/A')\n",
    "    \n",
    "    log_entry = {\n",
    "        'timestamp': timestamp,\n",
    "        'event': event,\n",
    "        'layer': layer,\n",
    "        'project': 'customer_analytics'\n",
    "    }\n",
    "    \n",
    "    with open(audit_log_path, 'a') as f:\n",
    "        f.write(json.dumps(log_entry) + '\\n')\n",
    "    \n",
    "    print(f\"[{timestamp}] {event} - {layer}\")\n",
    "\n",
    "# Data quality checker\n",
    "def quality_check(payload):\n",
    "    \"\"\"Check data quality after transformations.\"\"\"\n",
    "    df = payload.get('df')\n",
    "    if df is not None:\n",
    "        null_pct = df.isnull().sum().sum() / (len(df) * len(df.columns))\n",
    "        print(f\"  📊 Quality Check: {len(df)} rows, {null_pct:.2%} nulls\")\n",
    "\n",
    "# Register hooks\n",
    "project_hooks.register(\"pre_transform\", audit_logger)\n",
    "project_hooks.register(\"post_transform\", audit_logger)\n",
    "project_hooks.register(\"post_transform\", quality_check)\n",
    "\n",
    "print(\"✓ Observability configured\")\n",
    "print(f\"  Audit log: {audit_log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Execute Pipeline (Pandas Engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi_de_v2.transformer import TransformationRunnerFromConfig\n",
    "from odibi_de_v2.logger import MetadataManager, DynamicLogger\n",
    "\n",
    "# Initialize metadata and logging\n",
    "metadata_mgr = MetadataManager(connection_string=None)  # In-memory\n",
    "logger = DynamicLogger(\"customer_analytics\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXECUTING CUSTOMER ANALYTICS PIPELINE (Pandas Engine)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Stage 1: Clean Transactions\n",
    "print(\"\\n[STAGE 1/3] Silver - Clean Transactions\")\n",
    "print(\"-\" * 70)\n",
    "project_hooks.emit(\"pre_transform\", {\"event\": \"pre_transform\", \"layer\": \"silver\"})\n",
    "\n",
    "silver_txn_runner = TransformationRunnerFromConfig(\n",
    "    config=silver_transactions_config,\n",
    "    metadata_manager=metadata_mgr,\n",
    "    logger=logger,\n",
    "    engine=\"pandas\"\n",
    ")\n",
    "silver_txn_runner.run()\n",
    "\n",
    "# Load and verify\n",
    "transactions_silver = pd.read_csv(project_root / \"data/silver/transactions_clean.csv\")\n",
    "project_hooks.emit(\"post_transform\", {\n",
    "    \"event\": \"post_transform\",\n",
    "    \"layer\": \"silver\",\n",
    "    \"df\": transactions_silver\n",
    "})\n",
    "\n",
    "print(f\"\\n✓ Transactions cleaned: {len(transactions_silver)} rows\")\n",
    "print(transactions_silver.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Clean Customers\n",
    "print(\"\\n[STAGE 2/3] Silver - Clean Customers\")\n",
    "print(\"-\" * 70)\n",
    "project_hooks.emit(\"pre_transform\", {\"event\": \"pre_transform\", \"layer\": \"silver\"})\n",
    "\n",
    "silver_cust_runner = TransformationRunnerFromConfig(\n",
    "    config=silver_customers_config,\n",
    "    metadata_manager=metadata_mgr,\n",
    "    logger=logger,\n",
    "    engine=\"pandas\"\n",
    ")\n",
    "silver_cust_runner.run()\n",
    "\n",
    "customers_silver = pd.read_csv(project_root / \"data/silver/customers_clean.csv\")\n",
    "project_hooks.emit(\"post_transform\", {\n",
    "    \"event\": \"post_transform\",\n",
    "    \"layer\": \"silver\",\n",
    "    \"df\": customers_silver\n",
    "})\n",
    "\n",
    "print(f\"\\n✓ Customers cleaned: {len(customers_silver)} rows\")\n",
    "print(customers_silver.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3: Calculate Customer Metrics\n",
    "print(\"\\n[STAGE 3/3] Gold - Customer Metrics\")\n",
    "print(\"-\" * 70)\n",
    "project_hooks.emit(\"pre_transform\", {\"event\": \"pre_transform\", \"layer\": \"gold\"})\n",
    "\n",
    "gold_runner = TransformationRunnerFromConfig(\n",
    "    config=gold_metrics_config,\n",
    "    metadata_manager=metadata_mgr,\n",
    "    logger=logger,\n",
    "    engine=\"pandas\"\n",
    ")\n",
    "gold_runner.run()\n",
    "\n",
    "customer_metrics = pd.read_csv(project_root / \"data/gold/customer_metrics.csv\")\n",
    "project_hooks.emit(\"post_transform\", {\n",
    "    \"event\": \"post_transform\",\n",
    "    \"layer\": \"gold\",\n",
    "    \"df\": customer_metrics\n",
    "})\n",
    "\n",
    "print(f\"\\n✓ Customer metrics calculated: {len(customer_metrics)} customers\")\n",
    "print(\"\\nTop 10 Customers by Revenue:\")\n",
    "print(customer_metrics[[\n",
    "    'customer_id', 'total_transactions', 'total_revenue',\n",
    "    'avg_order_value', 'value_segment'\n",
    "]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business insights\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUSINESS INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Join customer metrics with customer master data\n",
    "customer_analysis = customer_metrics.merge(\n",
    "    customers_silver[['customer_id', 'segment', 'tenure_segment']],\n",
    "    on='customer_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Insight 1: Revenue by customer segment\n",
    "print(\"\\n1. Revenue by Customer Segment:\")\n",
    "segment_revenue = customer_analysis.groupby('segment').agg({\n",
    "    'customer_id': 'count',\n",
    "    'total_revenue': 'sum',\n",
    "    'avg_order_value': 'mean'\n",
    "}).round(2)\n",
    "segment_revenue.columns = ['Customers', 'Total Revenue', 'Avg Order Value']\n",
    "print(segment_revenue)\n",
    "\n",
    "# Insight 2: Value distribution\n",
    "print(\"\\n2. Customer Value Distribution:\")\n",
    "value_dist = customer_metrics['value_segment'].value_counts().sort_index()\n",
    "print(value_dist)\n",
    "\n",
    "# Insight 3: High-value customers\n",
    "print(\"\\n3. High-Value Customer Characteristics:\")\n",
    "high_value = customer_analysis[customer_analysis['value_segment'] == 'High']\n",
    "print(f\"   Count: {len(high_value)}\")\n",
    "print(f\"   Avg Transactions: {high_value['total_transactions'].mean():.1f}\")\n",
    "print(f\"   Avg Revenue: ${high_value['total_revenue'].mean():.2f}\")\n",
    "print(f\"   Segment Distribution:\\n{high_value['segment'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Orchestration Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./customer_analytics_project/run_pipeline.py\n",
    "\"\"\"\n",
    "Customer Analytics Pipeline Orchestrator\n",
    "\n",
    "Usage:\n",
    "    python run_pipeline.py --engine pandas\n",
    "    python run_pipeline.py --engine spark\n",
    "\"\"\"\n",
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project to path\n",
    "project_root = Path(__file__).parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from odibi_de_v2.transformer import TransformationRunnerFromConfig\n",
    "from odibi_de_v2.logger import MetadataManager, DynamicLogger\n",
    "from odibi_de_v2.config import TransformationConfig\n",
    "from odibi_de_v2.hooks import HookManager\n",
    "\n",
    "\n",
    "def setup_configs():\n",
    "    \"\"\"Create transformation configurations.\"\"\"\n",
    "    configs = {\n",
    "        'silver_transactions': TransformationConfig(\n",
    "            project=\"customer_analytics\",\n",
    "            plant=\"all\",\n",
    "            asset=\"transactions\",\n",
    "            module=\"functions.silver.transactions\",\n",
    "            function=\"clean_transactions\",\n",
    "            input_table=str(project_root / \"data/bronze/transactions_raw.csv\"),\n",
    "            target_table=str(project_root / \"data/silver/transactions_clean.csv\"),\n",
    "            enabled=True,\n",
    "            env=\"dev\",\n",
    "            layer=\"silver\"\n",
    "        ),\n",
    "        'silver_customers': TransformationConfig(\n",
    "            project=\"customer_analytics\",\n",
    "            plant=\"all\",\n",
    "            asset=\"customers\",\n",
    "            module=\"functions.silver.customers\",\n",
    "            function=\"clean_customers\",\n",
    "            input_table=str(project_root / \"data/bronze/customers_raw.csv\"),\n",
    "            target_table=str(project_root / \"data/silver/customers_clean.csv\"),\n",
    "            enabled=True,\n",
    "            env=\"dev\",\n",
    "            layer=\"silver\"\n",
    "        ),\n",
    "        'gold_metrics': TransformationConfig(\n",
    "            project=\"customer_analytics\",\n",
    "            plant=\"all\",\n",
    "            asset=\"customer_metrics\",\n",
    "            module=\"functions.gold.customer_metrics\",\n",
    "            function=\"calculate_customer_metrics\",\n",
    "            input_table=str(project_root / \"data/silver/transactions_clean.csv\"),\n",
    "            target_table=str(project_root / \"data/gold/customer_metrics.csv\"),\n",
    "            enabled=True,\n",
    "            env=\"dev\",\n",
    "            layer=\"gold\"\n",
    "        )\n",
    "    }\n",
    "    return configs\n",
    "\n",
    "\n",
    "def run_pipeline(engine='pandas'):\n",
    "    \"\"\"Execute the complete pipeline.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CUSTOMER ANALYTICS PIPELINE ({engine.upper()} ENGINE)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Setup\n",
    "    metadata_mgr = MetadataManager(connection_string=None)\n",
    "    logger = DynamicLogger(\"customer_analytics\")\n",
    "    configs = setup_configs()\n",
    "    \n",
    "    # Execute stages\n",
    "    stages = [\n",
    "        ('Silver - Transactions', configs['silver_transactions']),\n",
    "        ('Silver - Customers', configs['silver_customers']),\n",
    "        ('Gold - Metrics', configs['gold_metrics'])\n",
    "    ]\n",
    "    \n",
    "    for idx, (stage_name, config) in enumerate(stages, 1):\n",
    "        print(f\"[{idx}/{len(stages)}] {stage_name}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        runner = TransformationRunnerFromConfig(\n",
    "            config=config,\n",
    "            metadata_manager=metadata_mgr,\n",
    "            logger=logger,\n",
    "            engine=engine\n",
    "        )\n",
    "        runner.run()\n",
    "        print(f\"✓ {stage_name} complete\\n\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PIPELINE COMPLETE\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Run Customer Analytics Pipeline')\n",
    "    parser.add_argument(\n",
    "        '--engine',\n",
    "        choices=['pandas', 'spark'],\n",
    "        default='pandas',\n",
    "        help='Execution engine (default: pandas)'\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    run_pipeline(engine=args.engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Create README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./customer_analytics_project/README.md\n",
    "# Customer Analytics Pipeline\n",
    "\n",
    "End-to-end data pipeline for customer purchase behavior analysis built with odibi_de_v2.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Goal:** Identify high-value customers and analyze purchasing patterns.\n",
    "\n",
    "**Architecture:** Medallion (Bronze → Silver → Gold)\n",
    "\n",
    "**Engines Supported:** Pandas (local) and Spark (distributed)\n",
    "\n",
    "## Directory Structure\n",
    "\n",
    "```\n",
    "customer_analytics_project/\n",
    "├── config/                    # Configuration files\n",
    "├── data/\n",
    "│   ├── bronze/               # Raw data\n",
    "│   ├── silver/               # Cleaned data\n",
    "│   └── gold/                 # Aggregated metrics\n",
    "├── functions/\n",
    "│   ├── silver/               # Silver layer transformations\n",
    "│   └── gold/                 # Gold layer aggregations\n",
    "├── notebooks/                # Analysis notebooks\n",
    "├── logs/                     # Audit and execution logs\n",
    "├── run_pipeline.py           # Main orchestration script\n",
    "└── README.md                 # This file\n",
    "```\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install odibi_de_v2 pandas\n",
    "# Optional for Spark:\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "### Run Pipeline\n",
    "\n",
    "```bash\n",
    "# Using Pandas engine (default)\n",
    "python run_pipeline.py\n",
    "\n",
    "# Using Spark engine\n",
    "python run_pipeline.py --engine spark\n",
    "```\n",
    "\n",
    "## Pipeline Stages\n",
    "\n",
    "### 1. Bronze Layer\n",
    "- **transactions_raw.csv**: Raw transaction records\n",
    "- **customers_raw.csv**: Customer master data\n",
    "\n",
    "### 2. Silver Layer\n",
    "- **transactions_clean.csv**: Deduplicated, filtered, enriched transactions\n",
    "- **customers_clean.csv**: Standardized customer data with tenure\n",
    "\n",
    "### 3. Gold Layer\n",
    "- **customer_metrics.csv**: Aggregated KPIs per customer\n",
    "\n",
    "## Key Metrics\n",
    "\n",
    "- Total Revenue per Customer\n",
    "- Transaction Count\n",
    "- Average Order Value\n",
    "- Customer Lifetime Value\n",
    "- Recency (days since last purchase)\n",
    "- Customer Segments (Low/Medium/High value)\n",
    "\n",
    "## Customization\n",
    "\n",
    "### Adding New Transformations\n",
    "\n",
    "1. Create function in `functions/[layer]/`\n",
    "2. Register with `@pandas_function` or `@spark_function`\n",
    "3. Add config to `run_pipeline.py`\n",
    "4. Execute pipeline\n",
    "\n",
    "### Switching Engines\n",
    "\n",
    "The framework automatically selects the right function variant based on `--engine` flag.\n",
    "\n",
    "## Monitoring\n",
    "\n",
    "- Audit logs: `logs/pipeline_audit.log`\n",
    "- Data quality checks run automatically\n",
    "- Metadata tracked in MetadataManager\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. ✓ Always version transformation functions\n",
    "2. ✓ Use constants for configuration parameters\n",
    "3. ✓ Register hooks for observability\n",
    "4. ✓ Test with Pandas before scaling to Spark\n",
    "5. ✓ Document business logic in docstrings\n",
    "\n",
    "## Support\n",
    "\n",
    "See odibi_de_v2 documentation: `docs/tutorials/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Project Template Created:**\n",
    "\n",
    "✓ **Complete directory structure** with data, functions, config, and logs\n",
    "\n",
    "✓ **Medallion architecture** (Bronze → Silver → Gold)\n",
    "\n",
    "✓ **Dual-engine support** (Pandas and Spark variants)\n",
    "\n",
    "✓ **Transformation functions** with `@pandas_function` decorators\n",
    "\n",
    "✓ **Configuration management** using TransformationConfig\n",
    "\n",
    "✓ **Observability** with hooks and audit logging\n",
    "\n",
    "✓ **Orchestration script** (`run_pipeline.py`)\n",
    "\n",
    "✓ **Documentation** (README.md)\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. Adapt this template for your domain (replace customer analytics with your use case)\n",
    "2. Add more layers or transformations as needed\n",
    "3. Integrate with your data sources (databases, APIs, cloud storage)\n",
    "4. Add automated testing (see `tests/` directory)\n",
    "5. Deploy to production (Databricks, Airflow, etc.)\n",
    "\n",
    "**Key Learnings:**\n",
    "\n",
    "- Structure projects with clear layer separation\n",
    "- Use function registry for reusable transformations\n",
    "- Implement observability from day one\n",
    "- Design for both local (Pandas) and distributed (Spark) execution\n",
    "- Configuration-driven pipelines enable flexibility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
