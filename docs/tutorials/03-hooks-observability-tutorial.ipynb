{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Hooks and Observability\n",
    "\n",
    "Learn how to register lifecycle hooks for monitoring, logging, and custom behaviors in your data pipelines.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand pipeline lifecycle events\n",
    "- Register hooks for different stages (pre_read, post_transform, etc.)\n",
    "- Use filters to target specific projects/layers/engines\n",
    "- Build custom observability (logging, metrics, alerts)\n",
    "- Debug pipelines with hook introspection\n",
    "\n",
    "**Prerequisites:**\n",
    "- odibi_de_v2 installed\n",
    "- Completion of Tutorial 1 recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Lifecycle Events\n",
    "\n",
    "**Standard Events:**\n",
    "- `pre_read`: Before data ingestion\n",
    "- `post_read`: After data is loaded\n",
    "- `pre_transform`: Before transformation logic\n",
    "- `post_transform`: After transformation completes\n",
    "- `pre_save`: Before writing output\n",
    "- `post_save`: After data is saved\n",
    "- `on_error`: When an exception occurs\n",
    "- `pipeline_start`: Beginning of orchestration\n",
    "- `pipeline_end`: End of orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi_de_v2.hooks import HookManager\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a hook manager\n",
    "hooks = HookManager()\n",
    "\n",
    "# Simple logging hook\n",
    "def log_event(payload):\n",
    "    event_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    event_type = payload.get('event', 'unknown')\n",
    "    print(f\"[{event_time}] Event: {event_type}\")\n",
    "\n",
    "# Register for multiple events\n",
    "hooks.register(\"pre_read\", log_event)\n",
    "hooks.register(\"post_read\", log_event)\n",
    "hooks.register(\"pre_transform\", log_event)\n",
    "hooks.register(\"post_transform\", log_event)\n",
    "\n",
    "print(\"‚úì Registered logging hooks for pipeline events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate pipeline execution\n",
    "print(\"\\nSimulating Pipeline Execution:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "hooks.emit(\"pre_read\", {\"event\": \"pre_read\", \"table\": \"bronze.raw_data\"})\n",
    "print(\"  ‚Üí Loading data...\")\n",
    "\n",
    "hooks.emit(\"post_read\", {\"event\": \"post_read\", \"rows\": 1000})\n",
    "print(\"  ‚Üí Data loaded successfully\")\n",
    "\n",
    "hooks.emit(\"pre_transform\", {\"event\": \"pre_transform\", \"function\": \"clean_data\"})\n",
    "print(\"  ‚Üí Running transformation...\")\n",
    "\n",
    "hooks.emit(\"post_transform\", {\"event\": \"post_transform\", \"rows_out\": 950})\n",
    "print(\"  ‚Üí Transformation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Filtered Hooks\n",
    "\n",
    "Use filters to target specific projects, layers, or engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new hook manager for this example\n",
    "filtered_hooks = HookManager()\n",
    "\n",
    "# Hook that only runs for bronze layer\n",
    "def bronze_validation(payload):\n",
    "    print(f\"  üîç Running bronze layer validation\")\n",
    "    df = payload.get('df')\n",
    "    if df is not None:\n",
    "        print(f\"     Rows: {len(df)}\")\n",
    "\n",
    "filtered_hooks.register(\n",
    "    \"post_read\",\n",
    "    bronze_validation,\n",
    "    filters={\"layer\": \"bronze\"}\n",
    ")\n",
    "\n",
    "# Hook that only runs for silver layer\n",
    "def silver_quality_check(payload):\n",
    "    print(f\"  ‚úì Running silver layer quality checks\")\n",
    "\n",
    "filtered_hooks.register(\n",
    "    \"post_transform\",\n",
    "    silver_quality_check,\n",
    "    filters={\"layer\": \"silver\"}\n",
    ")\n",
    "\n",
    "# Hook that only runs for Spark engine\n",
    "def spark_optimization(payload):\n",
    "    print(f\"  ‚ö° Applying Spark-specific optimizations\")\n",
    "\n",
    "filtered_hooks.register(\n",
    "    \"pre_transform\",\n",
    "    spark_optimization,\n",
    "    filters={\"engine\": \"spark\"}\n",
    ")\n",
    "\n",
    "print(\"‚úì Registered filtered hooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Test with different layer payloads\n",
    "print(\"\\nTest 1: Bronze Layer\")\n",
    "print(\"-\" * 50)\n",
    "sample_df = pd.DataFrame({'id': [1, 2, 3], 'value': [10, 20, 30]})\n",
    "filtered_hooks.emit(\"post_read\", {\"layer\": \"bronze\", \"df\": sample_df})\n",
    "\n",
    "print(\"\\nTest 2: Silver Layer\")\n",
    "print(\"-\" * 50)\n",
    "filtered_hooks.emit(\"post_transform\", {\"layer\": \"silver\"})\n",
    "\n",
    "print(\"\\nTest 3: Gold Layer (no matching hooks)\")\n",
    "print(\"-\" * 50)\n",
    "filtered_hooks.emit(\"post_transform\", {\"layer\": \"gold\"})\n",
    "print(\"  (No hooks executed - filter mismatch)\")\n",
    "\n",
    "print(\"\\nTest 4: Spark Engine\")\n",
    "print(\"-\" * 50)\n",
    "filtered_hooks.emit(\"pre_transform\", {\"engine\": \"spark\", \"layer\": \"silver\"})\n",
    "\n",
    "print(\"\\nTest 5: Pandas Engine (no Spark hook)\")\n",
    "print(\"-\" * 50)\n",
    "filtered_hooks.emit(\"pre_transform\", {\"engine\": \"pandas\", \"layer\": \"silver\"})\n",
    "print(\"  (Spark optimization hook skipped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Validation Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_hooks = HookManager()\n",
    "\n",
    "# Schema validation hook\n",
    "def validate_schema(payload):\n",
    "    \"\"\"Ensure required columns exist.\"\"\"\n",
    "    df = payload.get('df')\n",
    "    required_columns = payload.get('required_columns', [])\n",
    "    \n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    missing = [col for col in required_columns if col not in df.columns]\n",
    "    \n",
    "    if missing:\n",
    "        raise ValueError(f\"‚ùå Schema validation failed. Missing columns: {missing}\")\n",
    "    else:\n",
    "        print(f\"  ‚úì Schema validation passed: {required_columns}\")\n",
    "\n",
    "validation_hooks.register(\"post_read\", validate_schema)\n",
    "\n",
    "# Null check hook\n",
    "def check_nulls(payload):\n",
    "    \"\"\"Alert if null percentage exceeds threshold.\"\"\"\n",
    "    df = payload.get('df')\n",
    "    threshold = payload.get('null_threshold', 0.1)\n",
    "    \n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    total_cells = len(df) * len(df.columns)\n",
    "    null_count = df.isnull().sum().sum()\n",
    "    null_pct = null_count / total_cells if total_cells > 0 else 0\n",
    "    \n",
    "    if null_pct > threshold:\n",
    "        print(f\"  ‚ö†Ô∏è  High null percentage: {null_pct:.2%} (threshold: {threshold:.2%})\")\n",
    "    else:\n",
    "        print(f\"  ‚úì Null check passed: {null_pct:.2%}\")\n",
    "\n",
    "validation_hooks.register(\"post_read\", check_nulls)\n",
    "\n",
    "print(\"‚úì Registered validation hooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test validation with good data\n",
    "print(\"\\nValidation Test 1: Good Data\")\n",
    "print(\"=\" * 50)\n",
    "good_df = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'score': [95, 87, 92, 88]\n",
    "})\n",
    "\n",
    "validation_hooks.emit(\"post_read\", {\n",
    "    \"df\": good_df,\n",
    "    \"required_columns\": [\"id\", \"name\", \"score\"],\n",
    "    \"null_threshold\": 0.1\n",
    "})\n",
    "\n",
    "# Test validation with missing column\n",
    "print(\"\\nValidation Test 2: Missing Column\")\n",
    "print(\"=\" * 50)\n",
    "try:\n",
    "    validation_hooks.emit(\"post_read\", {\n",
    "        \"df\": good_df,\n",
    "        \"required_columns\": [\"id\", \"name\", \"score\", \"department\"],\n",
    "        \"null_threshold\": 0.1\n",
    "    })\n",
    "except ValueError as e:\n",
    "    print(str(e))\n",
    "\n",
    "# Test validation with high nulls\n",
    "print(\"\\nValidation Test 3: High Null Percentage\")\n",
    "print(\"=\" * 50)\n",
    "bad_df = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'name': ['Alice', None, None, 'Diana'],\n",
    "    'score': [95, None, None, 88]\n",
    "})\n",
    "\n",
    "validation_hooks.emit(\"post_read\", {\n",
    "    \"df\": bad_df,\n",
    "    \"required_columns\": [\"id\", \"name\", \"score\"],\n",
    "    \"null_threshold\": 0.1\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Metrics and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Metrics collector\n",
    "class PipelineMetrics:\n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(dict)\n",
    "        self.timers = {}\n",
    "    \n",
    "    def start_timer(self, stage):\n",
    "        self.timers[stage] = time.time()\n",
    "    \n",
    "    def stop_timer(self, stage):\n",
    "        if stage in self.timers:\n",
    "            duration = time.time() - self.timers[stage]\n",
    "            self.metrics[stage]['duration_seconds'] = duration\n",
    "            del self.timers[stage]\n",
    "    \n",
    "    def record(self, stage, key, value):\n",
    "        self.metrics[stage][key] = value\n",
    "    \n",
    "    def report(self):\n",
    "        print(\"\\nüìä Pipeline Metrics Report\")\n",
    "        print(\"=\" * 60)\n",
    "        for stage, data in self.metrics.items():\n",
    "            print(f\"\\n{stage}:\")\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, float):\n",
    "                    print(f\"  {key}: {value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create metrics instance\n",
    "metrics = PipelineMetrics()\n",
    "metrics_hooks = HookManager()\n",
    "\n",
    "# Hook to track read timing\n",
    "def track_read_start(payload):\n",
    "    metrics.start_timer(\"read\")\n",
    "    print(\"  ‚è±Ô∏è  Starting read timer\")\n",
    "\n",
    "def track_read_end(payload):\n",
    "    metrics.stop_timer(\"read\")\n",
    "    df = payload.get('df')\n",
    "    if df is not None:\n",
    "        metrics.record(\"read\", \"rows_read\", len(df))\n",
    "        metrics.record(\"read\", \"columns\", len(df.columns))\n",
    "    print(\"  ‚è±Ô∏è  Read complete\")\n",
    "\n",
    "# Hook to track transform timing\n",
    "def track_transform_start(payload):\n",
    "    metrics.start_timer(\"transform\")\n",
    "    print(\"  ‚è±Ô∏è  Starting transform timer\")\n",
    "\n",
    "def track_transform_end(payload):\n",
    "    metrics.stop_timer(\"transform\")\n",
    "    df_in = payload.get('df_in')\n",
    "    df_out = payload.get('df_out')\n",
    "    if df_in is not None:\n",
    "        metrics.record(\"transform\", \"rows_in\", len(df_in))\n",
    "    if df_out is not None:\n",
    "        metrics.record(\"transform\", \"rows_out\", len(df_out))\n",
    "        if df_in is not None:\n",
    "            pct_change = ((len(df_out) - len(df_in)) / len(df_in)) * 100\n",
    "            metrics.record(\"transform\", \"row_change_pct\", pct_change)\n",
    "    print(\"  ‚è±Ô∏è  Transform complete\")\n",
    "\n",
    "# Register hooks\n",
    "metrics_hooks.register(\"pre_read\", track_read_start)\n",
    "metrics_hooks.register(\"post_read\", track_read_end)\n",
    "metrics_hooks.register(\"pre_transform\", track_transform_start)\n",
    "metrics_hooks.register(\"post_transform\", track_transform_end)\n",
    "\n",
    "print(\"‚úì Registered metrics tracking hooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a pipeline with metrics\n",
    "print(\"\\nSimulating Pipeline with Metrics:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Read stage\n",
    "metrics_hooks.emit(\"pre_read\", {})\n",
    "time.sleep(0.1)  # Simulate read time\n",
    "raw_df = pd.DataFrame({\n",
    "    'id': range(1, 101),\n",
    "    'value': range(100, 200)\n",
    "})\n",
    "metrics_hooks.emit(\"post_read\", {\"df\": raw_df})\n",
    "\n",
    "# Transform stage\n",
    "metrics_hooks.emit(\"pre_transform\", {})\n",
    "time.sleep(0.05)  # Simulate transform time\n",
    "# Remove some rows\n",
    "transformed_df = raw_df[raw_df['value'] > 150]\n",
    "metrics_hooks.emit(\"post_transform\", {\"df_in\": raw_df, \"df_out\": transformed_df})\n",
    "\n",
    "# Display metrics\n",
    "metrics.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Error Handling Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_hooks = HookManager()\n",
    "\n",
    "# Error logging hook\n",
    "def log_error(payload):\n",
    "    error = payload.get('error')\n",
    "    stage = payload.get('stage', 'unknown')\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    print(f\"\\n‚ùå ERROR LOG [{timestamp}]\")\n",
    "    print(f\"   Stage: {stage}\")\n",
    "    print(f\"   Error Type: {type(error).__name__}\")\n",
    "    print(f\"   Message: {str(error)}\")\n",
    "    \n",
    "    # In production, this could send to logging service, Slack, etc.\n",
    "\n",
    "# Error notification hook\n",
    "def notify_on_error(payload):\n",
    "    project = payload.get('project', 'unknown')\n",
    "    layer = payload.get('layer', 'unknown')\n",
    "    \n",
    "    print(f\"\\nüìß ALERT: Pipeline failure in {project}/{layer}\")\n",
    "    # In production: send email, Slack message, PagerDuty alert, etc.\n",
    "\n",
    "error_hooks.register(\"on_error\", log_error)\n",
    "error_hooks.register(\"on_error\", notify_on_error)\n",
    "\n",
    "print(\"‚úì Registered error handling hooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate an error\n",
    "print(\"\\nSimulating Pipeline Error:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Simulate a transformation error\n",
    "    raise ValueError(\"Invalid data format: expected numeric column 'price'\")\n",
    "except Exception as e:\n",
    "    error_hooks.emit(\"on_error\", {\n",
    "        \"error\": e,\n",
    "        \"stage\": \"transformation\",\n",
    "        \"project\": \"retail_analytics\",\n",
    "        \"layer\": \"silver\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Debugging - List All Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive hook manager\n",
    "debug_hooks = HookManager()\n",
    "\n",
    "# Register various hooks\n",
    "debug_hooks.register(\"pre_read\", lambda p: None)\n",
    "debug_hooks.register(\"pre_read\", lambda p: None, filters={\"layer\": \"bronze\"})\n",
    "debug_hooks.register(\"post_read\", lambda p: None)\n",
    "debug_hooks.register(\"pre_transform\", lambda p: None, filters={\"engine\": \"spark\"})\n",
    "debug_hooks.register(\"post_transform\", lambda p: None)\n",
    "debug_hooks.register(\"post_transform\", lambda p: None, filters={\"layer\": \"gold\"})\n",
    "debug_hooks.register(\"on_error\", lambda p: None)\n",
    "\n",
    "# List all hooks\n",
    "all_hooks = debug_hooks.list_hooks()\n",
    "\n",
    "print(\"Registered Hooks Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for event, hooks_list in all_hooks.items():\n",
    "    print(f\"\\n{event}: {len(hooks_list)} hook(s)\")\n",
    "    for idx, hook in enumerate(hooks_list, 1):\n",
    "        filters = hook['filters']\n",
    "        filter_str = f\" (filters: {filters})\" if filters else \" (no filters)\"\n",
    "        print(f\"  {idx}.{filter_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Complete Example - Production-Ready Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready hook manager with all observability\n",
    "production_hooks = HookManager()\n",
    "production_metrics = PipelineMetrics()\n",
    "\n",
    "# 1. Schema validation\n",
    "def prod_schema_check(payload):\n",
    "    df = payload.get('df')\n",
    "    required = ['id', 'timestamp', 'value']\n",
    "    if df is not None:\n",
    "        missing = [c for c in required if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "# 2. Data quality metrics\n",
    "def prod_quality_metrics(payload):\n",
    "    df = payload.get('df')\n",
    "    if df is not None:\n",
    "        null_pct = df.isnull().sum().sum() / (len(df) * len(df.columns))\n",
    "        dup_pct = df.duplicated().sum() / len(df)\n",
    "        production_metrics.record(\"quality\", \"null_pct\", null_pct)\n",
    "        production_metrics.record(\"quality\", \"duplicate_pct\", dup_pct)\n",
    "\n",
    "# 3. Performance tracking\n",
    "def prod_perf_start(payload):\n",
    "    stage = payload.get('stage', 'unknown')\n",
    "    production_metrics.start_timer(stage)\n",
    "\n",
    "def prod_perf_end(payload):\n",
    "    stage = payload.get('stage', 'unknown')\n",
    "    production_metrics.stop_timer(stage)\n",
    "\n",
    "# 4. Audit logging\n",
    "def prod_audit_log(payload):\n",
    "    event = payload.get('event')\n",
    "    user = payload.get('user', 'system')\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    print(f\"[AUDIT] {timestamp} | {user} | {event}\")\n",
    "\n",
    "# Register all production hooks\n",
    "production_hooks.register(\"post_read\", prod_schema_check)\n",
    "production_hooks.register(\"post_read\", prod_quality_metrics)\n",
    "production_hooks.register(\"pre_read\", prod_perf_start)\n",
    "production_hooks.register(\"post_read\", prod_perf_end)\n",
    "production_hooks.register(\"pipeline_start\", prod_audit_log)\n",
    "production_hooks.register(\"pipeline_end\", prod_audit_log)\n",
    "\n",
    "print(\"‚úì Registered production-ready hooks\")\n",
    "print(f\"\\nTotal hooks: {sum(len(v) for v in production_hooks.list_hooks().values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a production pipeline simulation\n",
    "print(\"\\nProduction Pipeline Execution:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "production_hooks.emit(\"pipeline_start\", {\n",
    "    \"event\": \"pipeline_start\",\n",
    "    \"user\": \"data_engineer\"\n",
    "})\n",
    "\n",
    "production_hooks.emit(\"pre_read\", {\"stage\": \"read\"})\n",
    "time.sleep(0.05)\n",
    "\n",
    "prod_data = pd.DataFrame({\n",
    "    'id': range(1, 51),\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=50),\n",
    "    'value': range(100, 150)\n",
    "})\n",
    "\n",
    "production_hooks.emit(\"post_read\", {\n",
    "    \"stage\": \"read\",\n",
    "    \"df\": prod_data\n",
    "})\n",
    "\n",
    "production_hooks.emit(\"pipeline_end\", {\n",
    "    \"event\": \"pipeline_end\",\n",
    "    \"user\": \"data_engineer\"\n",
    "})\n",
    "\n",
    "production_metrics.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What You Learned:**\n",
    "\n",
    "1. ‚úì **Lifecycle Events**: pre_read, post_read, pre_transform, post_transform, pre_save, post_save, on_error\n",
    "2. ‚úì **Hook Registration**: `hooks.register(event, callback, filters={})`\n",
    "3. ‚úì **Filtered Execution**: Target specific layers, projects, or engines\n",
    "4. ‚úì **Data Validation**: Schema checks, null checks, quality gates\n",
    "5. ‚úì **Metrics & Monitoring**: Timing, row counts, performance tracking\n",
    "6. ‚úì **Error Handling**: Logging and alerting on failures\n",
    "7. ‚úì **Debugging**: List and inspect registered hooks\n",
    "\n",
    "**Key Patterns:**\n",
    "\n",
    "```python\n",
    "# Create manager\n",
    "hooks = HookManager()\n",
    "\n",
    "# Register hook\n",
    "def my_hook(payload):\n",
    "    print(payload)\n",
    "\n",
    "hooks.register(\"post_read\", my_hook, filters={\"layer\": \"bronze\"})\n",
    "\n",
    "# Emit event\n",
    "hooks.emit(\"post_read\", {\"layer\": \"bronze\", \"df\": data})\n",
    "```\n",
    "\n",
    "**Production Use Cases:**\n",
    "- **Monitoring**: Track pipeline performance and data quality\n",
    "- **Validation**: Enforce schema and business rules\n",
    "- **Alerting**: Send notifications on errors or anomalies\n",
    "- **Auditing**: Log all pipeline activities for compliance\n",
    "- **Debugging**: Add temporary hooks to investigate issues\n",
    "\n",
    "**Best Practices:**\n",
    "- Keep hooks lightweight (avoid heavy computation)\n",
    "- Use filters to minimize unnecessary executions\n",
    "- Log errors within hooks - don't let them fail silently\n",
    "- Use consistent payload structure across your pipelines\n",
    "\n",
    "**Next Steps:**\n",
    "- Tutorial 4: Complete Project Template"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
